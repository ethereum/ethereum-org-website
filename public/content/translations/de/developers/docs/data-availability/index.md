---
title: "Datenverfügbarkeit"
description: "Ein Überblick über Probleme und Lösungen im Zusammenhang mit der Datenverfügbarkeit bei Ethereum"
lang: de
---

"Don't trust, verify" ist eine gängige Maxime in Ethereum. Die Idee dahinter ist, dass Ihr Knoten unabhängig überprüfen kann, ob die Informationen, die er erhält, korrekt sind, indem er alle Transaktionen in den Blöcken, die er von Peers erhält, ausführt. Das stellt sicher, dass die vorgeschlagenen Änderungen genau mit denen übereinstimmen, die der Knoten unabhängig berechnet hat. Das bedeutet, dass die Knoten nicht darauf vertrauen müssen, dass die Absender des Blocks ehrlich sind. Dies ist nicht möglich, wenn Daten fehlen.

**Datenverfügbarkeit** bezieht sich auf das Vertrauen, das ein Benutzer haben kann, dass die zur Verifizierung eines Blocks erforderlichen Daten wirklich für alle Netzwerkteilnehmer verfügbar sind. Für Full Nodes auf Ethereum Layer 1 ist dies relativ einfach; der Full Node lädt eine Kopie aller Daten in jedem Block herunter – die Daten _müssen_ verfügbar sein, damit das Herunterladen möglich ist. Ein Block mit fehlenden Daten wird verworfen, anstatt der Blockchain hinzugefügt zu werden. Dies ist „Offchain Datenverfügbarkeit“ und ein Merkmal monolithischer Blockchains. Full Nodes können nicht dazu verleitet werden, ungültige Transaktionen zu akzeptieren, da sie jede Transaktion für sich selbst herunterladen und ausführen. Bei modularen Blockchains, Layer-2-Rollups und Light-Clients ist die Datenverfügbarkeits-Landschaft jedoch komplexer und erfordert ausgefeiltere Überprüfungsverfahren.

## Voraussetzungen {#prerequisites}

Sie sollten ein gutes Verständnis der [Blockchain-Grundlagen](/developers/docs/intro-to-ethereum/) haben, insbesondere der [Konsensmechanismen](/developers/docs/consensus-mechanisms/). Auf dieser Seite wird außerdem davon ausgegangen, dass die Leserschaft mit [Blöcken](/developers/docs/blocks/), [Transaktionen](/developers/docs/transactions/), [Nodes](/developers/docs/nodes-and-clients/), [Skalierungslösungen](/developers/docs/scaling/) und anderen relevanten Themen vertraut ist.

## Das Problem der Datenverfügbarkeit {#the-data-availability-problem}

Das Problem der Datenverfügbarkeit besteht darin, dass dem gesamten Netzwerk nachgewiesen werden muss, dass die zusammengefasste Form einiger Transaktionsdaten, die der Blockchain hinzugefügt werden, tatsächlich einen Satz gültiger Transaktionen darstellt, ohne dass dabei alle Knoten alle Daten herunterladen müssen. Die vollständigen Transaktionsdaten sind für die unabhängige Überprüfung von Blöcken erforderlich. Die Anforderung, dass alle Knoten alle Transaktionsdaten herunterladen müssen, stellt jedoch ein Hindernis für die Skalierung dar. Lösungen für das Problem der Datenverfügbarkeit zielen darauf ab, ausreichende Sicherheiten dafür zu bieten, dass den Netzwerkteilnehmern, die die Daten nicht selbst herunterladen und speichern, die vollständigen Transaktionsdaten zur Überprüfung zur Verfügung gestellt wurden.

[Light Nodes](/developers/docs/nodes-and-clients/light-clients) und [Layer-2-Rollups](/developers/docs/scaling) sind wichtige Beispiele für Netzwerkteilnehmer, die starke Garantien zur Datenverfügbarkeit benötigen, aber Transaktionsdaten nicht selbst herunterladen und verarbeiten können. Ein nicht durchgeführter Vorgang zum Herunterladen von Transaktionsdaten ist das, was Light Nodes leichtgewichtig macht und Rollups als echte Lösung für Skalierungsprobleme erscheinen lässt.

Die Datenverfügbarkeit ist auch ein kritisches Anliegen für zukünftige ["zustandslose"](/roadmap/statelessness) Ethereum-Clients, die keine Zustandsdaten herunterladen und speichern müssen, um Blöcke zu verifizieren. Die zustandslosen Clients müssen sich dennoch sicher sein, dass die Daten _irgendwo_ verfügbar sind und korrekt verarbeitet wurden.

## Lösungen für die Datenverfügbarkeit {#data-availability-solutions}

### Datenverfügbarkeits-Sampling (DAS) {#data-availability-sampling}

Das Data Availability Sampling (DAS) ist eine Überprüfungsmethode des Netzwerkes für die Datenverfügbarkeit, ohne dass die Arbeitsbelastung für jeden Knoten übermäßig hoch ist. Jeder Knoten (einschließlich der Knoten, die nicht für das Staking benannt wurden) ermöglicht das zufällige Herunterladen eines Teils des Datensatzes. Das erfolgreiche Herunterladen der Beispiele bestätigt mit hoher Sicherheit, dass alle Daten verfügbar sind. Dies beruht auf Erasure Coding (Fehlerkorrekturverfahren), das einen gegebenen Datensatz um redundante Informationen erweitert (dies geschieht, indem eine als _Polynom_ bekannte Funktion über die Daten angepasst und dieses Polynom an zusätzlichen Punkten ausgewertet wird). Dadurch können die ursprünglichen Daten bei Bedarf auf einem Datensatz, wo mehrere Kopien erfasst werden, erneut abgedeckt werden. Eine Konsequenz dieser Datenerstellung ist, dass, wenn _irgendwelche_ der ursprünglichen Daten nicht verfügbar sind, die _Hälfte_ der erweiterten Daten fehlen wird! Die Menge der von jedem Node heruntergeladenen Datenstichproben kann so angepasst werden, dass es _extrem_ wahrscheinlich ist, dass mindestens eines der von jedem Client abgetasteten Datenfragmente fehlt, _falls_ weniger als die Hälfte der Daten wirklich verfügbar ist.

DAS wird verwendet, um sicherzustellen, dass Rollup-Betreiber ihre Transaktionsdaten verfügbar machen, nachdem [Full Danksharding](/roadmap/danksharding/#what-is-danksharding) implementiert wurde. Unter Verwendung der redundanten Daten aus der obigen Tabelle als Beweis für die Existenz der Daten werden die zu beprobenden Transaktionsdaten von den Ethereum-Knoten zufällig ausgewählt, die Batch-Ladungen von Datengruppen einrichten. Diese Technik kann auch verwendet werden, um Blockproduzenten die Möglichkeit zu geben, ihre Daten zur Verfügung zu stellen, um «light clients» zu schützen. In ähnlicher Weise wäre bei der [Trennung von Blockvorschlagendem und -ersteller (PBS)](/roadmap/pbs) nur der Blockersteller verpflichtet, einen ganzen Block zu verarbeiten – andere Validatoren würden die Verifizierung mittels Datenverfügbarkeits-Sampling durchführen.

### Datenverfügbarkeitskomitees {#data-availability-committees}

Datenschutzausschüsse (DACs), sind Ausschüsse, die sich aus Interessengruppen zusammensetzen, sie betonen die Notwendigkeit, Informationen über die Verfügbarkeit von Daten bereitzustellen und zu gewährleisten. DACs können anstelle von [oder in Kombination mit](https://hackmd.io/@vbuterin/sharding_proposal#Why-not-use-just-committees-and-not-DAS) DAS verwendet werden. Die von den Ausschüssen gebotenen Sicherheitsgarantien hängen von ihrer Umsetzung ab. Ethereum verwendet zufällig ausgewählte Stichproben von Unterklassen von Prüfern, die die Verfügbarkeit von zum Beispiel «light nodes» zusichern.

DACs werden auch von einigen Validiums eingesetzt. Der DAC ist ein vertrauenswürdiger Satz von Knoten, der Datenkopien offline speichert. Die Umsetzung der DAC-Richtlinie wird für die Bereitstellung von Daten im Streitfall verlangt. Mitglieder des DAC veröffentlichen außerdem Offchain Bescheinigungen, um zu beweisen, dass die besagten Daten tatsächlich verfügbar sind. Einige Validiums ersetzen die DACs durch ein Proof-of-Stake (PoS)-Validierungssystem. Hier kann jeder zum Validator werden und Daten außerhalb der Kette speichern. Dennoch basieren die Anforderungen an die Datenübertragung auf einer "Vereinbarung", die in einem digitalen Protokoll hinterlegt ist. Im Falle einer böswilligen Absicht, wie zum Beispiel im Falle eines Datenlecks des Validators, könnte diese Vereinbarung gekündigt werden. Aufgrund ihrer Rolle, die ehrliches Verhalten fördert, verfügen Datenschutzausschüsse, die auf der Proof-of-Stake Methode basieren, über einen erheblich sichereren Speicherplatz als DACs.

## Datenverfügbarkeit und Light Nodes {#data-availability-and-light-nodes}

[Light Nodes](/developers/docs/nodes-and-clients/light-clients) müssen die Korrektheit der Block-Header, die sie empfangen, validieren, ohne die Blockdaten herunterzuladen. Diese Leichtigkeit hat ihren Preis: die Unfähigkeit, die Köpfe des Blocks (block headers) unabhängig zu überprüfen, indem neue Transaktionen lokal durchgeführt werden, wie es die Vollknoten derzeit tun.

Ethereum Light Nodes vertrauen auf zufällige Sätze von 512 Validatoren, die einem _Synchronisierungskomitee_ zugewiesen wurden. Das Sync-Komitee, das als programmierte Investition (DCA) fungiert, weist die «Light Clients» darauf hin, dass sie durch die Verwendung eines kryptografischen Algorithmus die im Header enthaltenen Daten validieren können. Der Beratungsausschuss führt tägliche Auffrischungen durch. Jeder Block-Header informiert Light Nodes darüber, von welchen Validatoren die Unterzeichnung des _nächsten_ Blocks zu erwarten ist, sodass sie nicht dazu verleitet werden können, einer bösartigen Gruppe zu vertrauen, die vorgibt, das echte Synchronisierungskomitee zu sein.

Was passiert jedoch, wenn es einem Angreifer irgendwie _doch_ gelingt, einen bösartigen Block-Header an Light Clients weiterzugeben und sie davon zu überzeugen, dass er von einem ehrlichen Synchronisierungskomitee unterzeichnet wurde? In diesem Fall könnte der Angreifer Transaktionen hinzufügen, die nicht gültig gemacht wurden, was den «Light Client» dazu veranlassen würde, ihnen blind zu vertrauen und sie zu validieren, da sie nicht in der Lage sind, die im Block Header aufgezeichneten Statusänderungen unabhängig zu überprüfen. Der «light client » kann Beweismittel anlegen, um sich vor Betrug zu schützen.

Wie werden diese Anfragen zur automatischen Reproduktion der Transaktion in der Praxis weitergegeben? Ein vollständiger Knoten stellt fest, dass im Netzwerk über einen ungültigen Zustandsübergang getratscht wird, und beschließt daher, sofort kleine Datendateien zu generieren, in denen der Beweis enthalten ist, dass der fragliche Zustandsübergang nicht Teil einer Reihe von eingereichten Transaktionen sein kann, und diese Daten an seine Peers weiterzuleiten. Die «light nodes» können hingehen und die Ergebnisse der letzten Anfrage zur automatischen Reproduktion der Transaktion (fraud proofs) abrufen, um bösartige Header zu erkennen. Dadurch wird sichergestellt, dass es sich bei den Akteuren, die an der Wartung der Kette beteiligt sind, um ehrliche Akteure handelt, wie es bei vollständigen Knoten der Fall ist.

Diese basieren auf vollständigen Knoten, die Zugriff auf die vollständigen Daten der Blöcke haben. Ein Angreifer, der einen bösartigen Block-Header übermittelt und es nicht schafft, die Transaktionsdaten verfügbar zu machen, wäre dann in der Lage, die Erstellung einer Anfrage zur automatischen Reproduktion der Transaktion durch die vollständigen Knoten zu verhindern. Die vollständigen Knoten könnten zwar eine Warnung vor einem fehlerhaften Block signalisieren, sie könnten ihre Warnung jedoch nicht mit Beweisen untermauern, da die Daten zur Generierung des Beweises nicht zur Verfügung gestellt wurden!

DAS: Die Lösung für das Problem der Datenverfügbarkeit. Die «light nodes» greifen auf vollständige Daten zurück, die aktualisiert wurden, um sehr kleine Blobs mit zufälligen Daten herunterladen zu können, die als Muster verwendet werden, um die Verfügbarkeit aller vollständigen Daten zu gewährleisten. Die tatsächliche Wahrscheinlichkeit, nach dem Herunterladen von N zufälligen Chunks fälschlicherweise von einer vollständigen Datenverfügbarkeit auszugehen, kann berechnet werden ([bei 100 Chunks liegt die Wahrscheinlichkeit bei 10^-30](https://dankradfeist.de/ethereum/2019/12/20/data-availability-checks.html), d. h. sie ist unglaublich gering).

Doch selbst unter diesem Szenario könnten Angriffe, die nur wenige Bytes zurückhalten, durchaus von den Kunden unbemerkt bleiben, die ihrerseits Abfragen mit zufälligen Daten stellen könnten. Löschen Coding behebt dieses Problem, indem es kleine fehlende Datenteile rekonstruiert, die zur Überprüfung vorgeschlagener Statusänderungen verwendet werden können. Mithilfe der rekonstruierten Daten könnte dann ein Betrugsnachweis erstellt werden, der verhindert, dass Light Nodes fehlerhafte Header akzeptieren.

**Hinweis:** DAS und Betrugsbeweise wurden für Proof-of-Stake-Ethereum-Light-Clients noch nicht implementiert, stehen aber auf der Roadmap und werden höchstwahrscheinlich die Form von ZK-SNARK-basierten Beweisen annehmen. Die heutigen Light-Clients verlassen sich auf eine Form von DAC: Sie überprüfen die Identitäten des Synchronisierungskomitees und vertrauen dann den signierten Blockheadern, die sie erhalten.

## Datenverfügbarkeit und Layer-2-Rollups {#data-availability-and-layer-2-rollups}

[Layer-2-Skalierungslösungen](/layer-2/) wie [Rollups](/glossary/#rollups) reduzieren die Transaktionskosten und erhöhen den Durchsatz von Ethereum durch die Verarbeitung von Transaktionen außerhalb der Chain. Rollup Transaktionen werden komprimiert und stapelweise auf Ethereum gepostet. Batches stellen Tausende einzelner Offchain Transaktionen in einer einzigen Transaktion auf Ethereum dar. Dies reduziert die Überlastung der Basisschicht und senkt die Gebühren für die Benutzer.

Nur wenn eine vorgeschlagene Zustandsänderung unabhängig verifiziert und als korrektes Ergebnis aller einzelnen Off-Chain-Transaktionen bestätigt werden kann, kann man den auf Ethereum geposteten „Sammeltransaktionen“ vertrauen. Falls die Rollup-Betreiber die Transaktionsdaten für diese Überprüfung nicht bereitstellen, besteht das Risiko, dass sie fehlerhafte Daten an Ethereum senden.

[Optimistic Rollups](/developers/docs/scaling/optimistic-rollups/) veröffentlichen komprimierte Transaktionsdaten auf Ethereum und warten eine gewisse Zeit (in der Regel 7 Tage), damit unabhängige Prüfer die Daten kontrollieren können. Sollte jemand ein Problem identifizieren, kann ein Betrugsnachweis (Fraud Proof) erstellt werden, um den Rollup anzufechten. Dies würde dazu führen, dass die Kette zurückgesetzt wird und der ungültige Block ausgelassen wird. Dies ist nur möglich, wenn Daten verfügbar sind. Derzeit gibt es zwei Möglichkeiten, wie optimistische Rollups Transaktionsdaten an L1 senden. Einige Rollups stellen Daten dauerhaft als `CALLDATA` zur Verfügung, die permanent auf der Chain gespeichert sind. Mit der Implementierung von EIP-4844 veröffentlichen einige Rollups ihre Transaktionsdaten stattdessen im günstigeren Klecks Speicher. Dies ist keine dauerhafte Speicherung. Bevor die Daten vom Ethereum-Layer-1 gelöscht werden, müssen unabhängige Verifizierer innerhalb von etwa 18 Tagen die Blobs prüfen und ihre Anfechtungen vorbringen. Die vom Ethereum-Protokoll garantierte Datenverfügbarkeit ist auf dieses kurze, feste Zeitfenster beschränkt. Anschließend sind andere Akteure im Ethereum-Ökosystem dafür verantwortlich. Jeder Node kann die Datenverfügbarkeit mithilfe von DAS überprüfen, d. h. durch Herunterladen kleiner, zufälliger Stichproben der Blob-Daten.

[Zero-Knowledge (ZK)-Rollups](/developers/docs/scaling/zk-rollups) müssen keine Transaktionsdaten veröffentlichen, da [Zero-Knowledge-Gültigkeitsnachweise](/glossary/#zk-proof) die Korrektheit von Zustandsübergängen garantieren. Die Datenverfügbarkeit bleibt jedoch weiterhin ein Problem, denn ohne Zugriff auf die State-Daten eines ZK-Rollups kann man dessen Funktionalität nicht gewährleisten oder damit interagieren. Hält ein Betreiber Details über den State des Rollups zurück, können Nutzer beispielsweise ihre Guthaben nicht einsehen. Zudem können sie keine State-Updates mithilfe von Informationen aus einem neu hinzugefügten Block durchführen.

## Datenverfügbarkeit vs. Datenabrufbarkeit {#data-availability-vs-data-retrievability}

Datenverfügbarkeit ist nicht dasselbe wie Datenabrufbarkeit. Datenverfügbarkeit bedeutet, dass Full Nodes in der Lage waren, auf alle Transaktionen eines bestimmten Blocks zuzugreifen und diese zu überprüfen. Dies ist jedoch nicht mit einer ewigen Verfügbarkeit der Daten gleichzusetzen.

Datenabrufbarkeit ist die Fähigkeit von Nodes, _historische Informationen_ von der Blockchain abzurufen. Für die Verifizierung neuer Blöcke werden diese historischen Daten nicht benötigt. Sie sind jedoch erforderlich, um Full Nodes ab dem Genesis-Block zu synchronisieren oder um spezifische historische Anfragen zu bedienen.

Für das Ethereum-Kernprotokoll steht nicht die Datenabrufbarkeit im Vordergrund, sondern primär die Datenverfügbarkeit. Die Datenabrufbarkeit kann von einer kleinen Population von Archiv-Nodes, die von Dritten betrieben werden, bereitgestellt oder über das Netzwerk mithilfe dezentraler Dateispeicher wie dem [Portal Network](https://www.ethportal.net/) verteilt werden.

## Weiterführende Lektüre {#further-reading}

- [WTF is Data Availability?](https://medium.com/blockchain-capital-blog/wtf-is-data-availability-80c2c95ded0f)
- [What Is Data Availability?](https://coinmarketcap.com/academy/article/what-is-data-availability)
- [A primer on data availability checks](https://dankradfeist.de/ethereum/2019/12/20/data-availability-checks.html)
- [An explanation of the sharding + DAS proposal](https://hackmd.io/@vbuterin/sharding_proposal#ELI5-data-availability-sampling)
- [A note on data availability and erasure coding](https://github.com/ethereum/research/wiki/A-note-on-data-availability-and-erasure-coding#can-an-attacker-not-circumvent-this-scheme-by-releasing-a-full-unavailable-block-but-then-only-releasing-individual-bits-of-data-as-clients-query-for-them)
- [Data availability committees.](https://medium.com/starkware/data-availability-e5564c416424)
- [Proof-of-stake data availability committees.](https://blog.matter-labs.io/zkporter-a-breakthrough-in-l2-scaling-ed5e48842fbf)
- [Solutions to the data retrievability problem](https://notes.ethereum.org/@vbuterin/data_sharding_roadmap#Who-would-store-historical-data-under-sharding)
- [Data Availability Or: How Rollups Learned To Stop Worrying And Love Ethereum](https://web.archive.org/web/20250515194659/https://web.archive.org/web/20241108192208/https://research.2077.xyz/data-availability-or-how-rollups-learned-to-stop-worrying-and-love-ethereum)
- [EIP-7623: Increasing Calldata Cost](https://web.archive.org/web/20250515194659/https://research.2077.xyz/eip-7623-increase-calldata-cost)
